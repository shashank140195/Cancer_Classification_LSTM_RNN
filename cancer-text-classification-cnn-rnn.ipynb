{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#################################### Libraries #######################################\n#%% libraries\nimport os # setting working directory\nimport numpy as np # for generating random embeddings\nimport pandas as pd # importing a csv\nimport random\nimport spacy # basic text processing\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.metrics import F1Score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nplt.style.use('ggplot')\nfrom statistics import harmonic_mean\nimport pickle\nfrom tensorflow.keras.layers import Embedding, LSTM, Conv1D, Bidirectional, Dense, Concatenate, GlobalMaxPooling1D, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# Red box below, after running, does not necessarily mean there is an error\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:55:59.744048Z","iopub.execute_input":"2021-12-01T03:55:59.744916Z","iopub.status.idle":"2021-12-01T03:55:59.755758Z","shell.execute_reply.started":"2021-12-01T03:55:59.744825Z","shell.execute_reply":"2021-12-01T03:55:59.754789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################################## preliminaries #################################\n# sets working directory\nos.chdir('/kaggle/input/cancer-data/')\n\n# sets pseudorandom seeds\nrandom.seed(0)\nnp.random.seed(0)\ntf.random.set_seed(0)    \n\n### TPU usage\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# Red box below, after running, does not necessarily mean there is an error","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:55:59.758161Z","iopub.execute_input":"2021-12-01T03:55:59.758691Z","iopub.status.idle":"2021-12-01T03:56:06.023980Z","shell.execute_reply.started":"2021-12-01T03:55:59.758638Z","shell.execute_reply":"2021-12-01T03:56:06.023039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################################### loads data ####################################\n# reading csv files of text for investigation\ntrain = pd.read_csv('training2_v1.csv')\ntest = pd.read_csv('testing1_v1.csv')\n\n# data for running neural networks\nx_large = pickle.load(open('x_training2', \"rb\"))\nx_test = pickle.load(open('x_testing1', \"rb\"))\ny_large = pickle.load(open('y_training2', \"rb\"))\ny_test = pickle.load(open('y_testing1', \"rb\"))\n\n# word embeddings\nembeddings_pretrained = pickle.load(open('embeddings_w2v', \"rb\" ) )\nembeddings_random = pickle.load(open('embeddings_random', \"rb\"))\n\n# maximum text sequence length\nmax_len = pickle.load(open('max_len', \"rb\"))\n\n# number and dimension of word embeddings\ndim_embed = embeddings_pretrained.shape[1]\nnum_embed = embeddings_pretrained.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.025517Z","iopub.execute_input":"2021-12-01T03:56:06.025813Z","iopub.status.idle":"2021-12-01T03:56:06.179507Z","shell.execute_reply.started":"2021-12-01T03:56:06.025780Z","shell.execute_reply":"2021-12-01T03:56:06.178536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################## train/validation splits #######################3\nx_train, x_valid, y_train, y_valid = train_test_split(x_large, y_large, test_size = 0.2, random_state = 0)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.181236Z","iopub.execute_input":"2021-12-01T03:56:06.182232Z","iopub.status.idle":"2021-12-01T03:56:06.191188Z","shell.execute_reply.started":"2021-12-01T03:56:06.182180Z","shell.execute_reply":"2021-12-01T03:56:06.190088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################## custom CNN with multiple filter window sizes ##############################\nclass Conv1D_multiple_filters(keras.layers.Layer):\n    # intializes class attributes\n    def __init__(self, filter_size_list, filter_num_list, activation, pooling_fun):\n        super().__init__()\n         \n        self.num_window_sizes = len(filter_size_list)\n        self.convolutions_list = [Conv1D(filter_num_list[i],\n                                   filter_size_list[i],\n                                   activation = activation)\n                             for i in range(self.num_window_sizes)\n                             ]\n        self.pooling_fun = pooling_fun()\n        self.concat = Concatenate()\n        \n    def call(self, x):\n        # runs n-grams through convolutions with activation functions\n        x = [self.convolutions_list[i](x) for i in range(self.num_window_sizes)]\n        \n        # pooling\n        x = [self.pooling_fun(x[i]) for i in range(self.num_window_sizes)]\n                \n        # concatenates results from different filter sizes.  If only bigrams are used, then there is not concatentation to be done.\n        if len(x) == 1:\n            x = x[0] # list of 1 tensor -> tensor\n        elif len(x) > 1:\n            x = self.concat(x) # concatentates list of >1 tensors to one tensor.\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.193637Z","iopub.execute_input":"2021-12-01T03:56:06.193978Z","iopub.status.idle":"2021-12-01T03:56:06.204220Z","shell.execute_reply.started":"2021-12-01T03:56:06.193948Z","shell.execute_reply":"2021-12-01T03:56:06.203309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% F-score metric\nclass F1(tf.keras.metrics.Metric):\n    # initializing class object\n    def __init__(self, name='F1', **kwargs):\n        super(F1, self).__init__(name=name, **kwargs)\n        \n        # intializes # TP's, FP's, and FN's to 0\n        self.TP = self.add_weight(name='TP', initializer='zeros')\n        self.FP = self.add_weight(name='FP', initializer='zeros')\n        self.FN = self.add_weight(name='FN', initializer='zeros')\n\n    # accumulates TP's, FP's, and FN's over batches in epoch\n    def update_state(self, y_true, y_pred, sample_weight = None):\n        \n        # converts probability score to boolean\n        y_pred = tf.where(y_pred > 0.5, True, False)\n\n        # ensures quantities are boolean\n        y_true = tf.cast(y_true, tf.bool)\n        y_pred = tf.cast(y_pred, tf.bool)\n        \n        # calculates # TP's, FP's, FN's in batch\n        TP_tensor = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n        TP_tensor = tf.cast(TP_tensor, self.dtype)\n        TP = tf.reduce_sum(TP_tensor)\n        \n        FP_tensor = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n        FP_tensor = tf.cast(FP_tensor, self.dtype)\n        FP = tf.reduce_sum(FP_tensor)\n        \n        FN_tensor = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n        FN_tensor = tf.cast(FN_tensor, self.dtype)\n        FN = tf.reduce_sum(FN_tensor)\n        \n        # adds TP's, FP's and FN's to those of previous batches in epoch\n        self.TP.assign_add(TP)\n        self.FP.assign_add(FP)\n        self.FN.assign_add(FN)\n    \n    # calculates F-score\n    def result(self):\n        \n        precision = tf.math.divide(self.TP, tf.math.add(self.TP, self.FP))\n        recall = tf.math.divide(self.TP, tf.math.add(self.TP, self.FN))\n        \n        numerator = 2 * tf.math.multiply(precision, recall)\n        denominator = tf.math.add(precision, recall)\n        \n        F1 = tf.math.divide(numerator, denominator)\n        \n        return F1\n    \n    # resets TP's, FP's, and FN's to 0 at the end of epoch\n    def reset_state(self):\n        self.TP.assign(0)\n        self.FP.assign(0)\n        self.FN.assign(0)\n        \n        \n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.205638Z","iopub.execute_input":"2021-12-01T03:56:06.206342Z","iopub.status.idle":"2021-12-01T03:56:06.223968Z","shell.execute_reply.started":"2021-12-01T03:56:06.206270Z","shell.execute_reply":"2021-12-01T03:56:06.222907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################## tracking model fitting ####################################\n# plots validation loss\ndef plot_loss(history):\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(val_loss) + 1)\n\n    plt.plot(epochs, val_loss, 'b')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Loss\")\n    plt.legend()\n\ndef plot_F1(history):\n    val_f1 = history.history['val_F1']\n    epochs = range(1, len(val_f1) + 1)\n\n    plt.plot(epochs, val_f1, 'b')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation F1\")\n    plt.legend()\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.225545Z","iopub.execute_input":"2021-12-01T03:56:06.225807Z","iopub.status.idle":"2021-12-01T03:56:06.240827Z","shell.execute_reply.started":"2021-12-01T03:56:06.225776Z","shell.execute_reply":"2021-12-01T03:56:06.239883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Modeling choices ##############################\n# choice between pretrained and random word embeddings (comment out one of the choices to use the other) - # student modifies this section\nembedding_matrix = embeddings_pretrained\n#embedding_matrix = embeddings_random\n\n# CNN hyperparameter choices - student modifies this section\nnum_filters = 800 # optional to modify\nmax_ngram = 3 # do 2,3,4,5\n\n# biLSTM hyperparameter choices  - student modifies this section\nrnn_dim_hidden = 256 # note this is for each of the two LSTMs\n\n# mini-batch size (optional to modify)\nbatch_size = 16\nnum_epochs = 100\n\n# dropout (optional to modify)\nprob_dropout_input = 0.8\nprob_dropout_after_NN = 0.5\nprob_dropout_rnn = 0.8\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.242479Z","iopub.execute_input":"2021-12-01T03:56:06.242887Z","iopub.status.idle":"2021-12-01T03:56:06.260532Z","shell.execute_reply.started":"2021-12-01T03:56:06.242859Z","shell.execute_reply":"2021-12-01T03:56:06.259733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################################### model definition #############################################\n# splits number of filters evenly into filter sizes, and if it doesn't divide evenly, adds one filter to smaller filter sizes until none left\nfilter_size_list = list(range(2, max_ngram + 1))\nfilter_num_list = [num_filters//(max_ngram - 1)] * (max_ngram - 1)\nremainder = num_filters % (max_ngram - 1)\nfor i in range(remainder):\n    filter_num_list[i] += 1\n\n# sets pseudorandom seeds\nrandom.seed(1)\nnp.random.seed(1)\ntf.random.set_seed(1)\n\n##### defining the model\nwith tpu_strategy.scope():\n\n    # using the Sequential module\n    model = Sequential()\n\n    # adds word embedding layer to model\n    model.add(Embedding(num_embed, dim_embed, \n                               weights=[embedding_matrix], \n                               input_length = max_len, \n                               trainable = True))\n\n    # dropout\n    model.add(Dropout(prob_dropout_input))\n\n    ### adds CNN or biLSM (choose by commenting out one of these lines) - student modifies this section\n    model.add(Conv1D_multiple_filters(filter_size_list, filter_num_list, activation = 'relu', pooling_fun = GlobalMaxPooling1D))\n    #model.add(Bidirectional(LSTM(rnn_dim_hidden, recurrent_dropout = prob_dropout_rnn)))\n    ###\n\n    # dropout\n    model.add(Dropout(prob_dropout_after_NN))\n\n    # adds a MLP(1) layer\n    model.add(Dense(1, activation = 'sigmoid'))\n\n\n    # chooses loss function, optimizer, and evaluation metrics\n    fscore = F1()\n    model.compile(optimizer = 'adam',\n                  loss = 'binary_crossentropy',\n                  metrics = ['Precision', 'Recall', fscore])\n\n    # prints model structure summary (just for user)\n    model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.261817Z","iopub.execute_input":"2021-12-01T03:56:06.262085Z","iopub.status.idle":"2021-12-01T03:56:06.827033Z","shell.execute_reply.started":"2021-12-01T03:56:06.262049Z","shell.execute_reply":"2021-12-01T03:56:06.826143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################################ model fitting ###############################\n\n# stops training when the model fails to improve validation loss for 20 epochs, and selects the best model \nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_F1',\n                                                  mode = 'max',\n                                                  patience = 20,\n                                                  restore_best_weights = True)\n\n\n# trains model\nhistory = model.fit(x_train, y_train,\n                    epochs = num_epochs,\n                    verbose = 1, # change to 0 if you don't want to print intermediate results during training\n                    validation_data = (x_valid, y_valid),\n                    batch_size = batch_size,\n                    validation_batch_size = len(y_valid),\n                    callbacks = [early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:56:06.828541Z","iopub.execute_input":"2021-12-01T03:56:06.829049Z","iopub.status.idle":"2021-12-01T03:59:10.813489Z","shell.execute_reply.started":"2021-12-01T03:56:06.829015Z","shell.execute_reply":"2021-12-01T03:59:10.812531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################### Performance #############################\n# prediction \ny_pred = np.round(model.predict(x_test)).flatten()\n\n# performance measures\nprecision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average = 'binary')\n\n# printing\nprint('precision : ', precision)\nprint('recall : ', recall)\nprint('F1 : ', f1)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:59:10.816576Z","iopub.execute_input":"2021-12-01T03:59:10.817583Z","iopub.status.idle":"2021-12-01T03:59:13.627738Z","shell.execute_reply.started":"2021-12-01T03:59:10.817527Z","shell.execute_reply":"2021-12-01T03:59:13.626660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#################### Plotting #################\nplot_loss(history)\nplt.show()\nplot_F1(history)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:59:13.629204Z","iopub.execute_input":"2021-12-01T03:59:13.630034Z","iopub.status.idle":"2021-12-01T03:59:14.058601Z","shell.execute_reply.started":"2021-12-01T03:59:13.629967Z","shell.execute_reply":"2021-12-01T03:59:14.057584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### accessing examples for error analysis\n\n# obtains list of false positive and negative indices\nFPs = [i for i in range(len(y_test)) if y_pred[i] == 1 and y_test[i] == 0]\nFNs = [i for i in range(len(y_test)) if y_pred[i] == 0 and y_test[i] == 1]\n\n# prints out the first several indices\nprint('FP: ', FPs[0:5])\nprint('FN: ', FNs[0:5])\n\n# example index\ni = 167\n\n# example label\nprint('label: ', y_test[i])\n\n# example predictions\nprint('prediction: ', y_pred[i])\n\n# example text\nprint(test['text'].loc[i])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:59:14.059863Z","iopub.execute_input":"2021-12-01T03:59:14.060138Z","iopub.status.idle":"2021-12-01T03:59:14.073964Z","shell.execute_reply.started":"2021-12-01T03:59:14.060108Z","shell.execute_reply":"2021-12-01T03:59:14.073128Z"},"trusted":true},"execution_count":null,"outputs":[]}]}